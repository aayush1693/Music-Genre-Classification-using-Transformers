# -*- coding: utf-8 -*-
"""exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/aayush1693/Music-Genre-Classification-using-Transformers/blob/main/exploration.ipynb

Step-by-step implementation:


---


Installing required module
At first, we need to install transformers, accelerate, datasets and evaluate modules to our runtime.
"""

!pip install transformers
!pip install accelerate
!pip install datasets
!pip install evaluate

"""Importing required libraries

---


Now we will import all required Python libraries like NumPy and transformers etc.
"""

from datasets import load_dataset, Audio
import numpy as np
from transformers import pipeline, AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer
import evaluate

"""Loading dataset and Splitting


---


Now we will load the GTZAN dataset which contains total 10 music genres. Then we will split it into training and testing sets(90:10).
"""

gtzan = load_dataset("marsyas/gtzan", "all")
gtzan = gtzan["train"].train_test_split(seed=42, shuffle=True, test_size=0.1)

"""Data pre-processing

---


Now we will extract the features of audio files using transformers’ AutoFeatureExtractor. And define a driver function to iterate over the audio files(.wav).

Model and Feature Initialization
  *   used a pretrained model from the Hugging Face model hub
  *   initialized the feature extractor

Load data and performed audio preprocessing

Preprocessed the audio data in the GTZAN dataset using the feature extractor, the preprocess_function applies the feature extractor to a list of audio arrays, setting options such as ‘max_length’ and ‘truncation’.



"""

model_id = "ntu-spml/distilhubert"
feature_extractor = AutoFeatureExtractor.from_pretrained(
    model_id, do_normalize=True, return_attention_mask=True
)
sampling_rate = feature_extractor.sampling_rate
gtzan = gtzan.cast_column("audio", Audio(sampling_rate=sampling_rate))
sample = gtzan["train"][0]["audio"]
inputs = feature_extractor(
    sample["array"], sampling_rate=sample["sampling_rate"])
max_duration = 20.0


def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=feature_extractor.sampling_rate,
        max_length=int(feature_extractor.sampling_rate * max_duration),
        truncation=True,
        return_attention_mask=True,
    )
    return inputs


gtzan_encoded = gtzan.map(
    preprocess_function,
    remove_columns=["audio", "file"],
    batched=True,
    batch_size=25,
    num_proc=1,
)

"""Encoding dataset:

---


To feed the dataset to the model we need to encode it.

*   Renamed the ‘genre’ column to ‘label’
*   Created mapping functions





"""

gtzan_encoded = gtzan_encoded.rename_column("genre", "label")
id2label_fn = gtzan["train"].features["genre"].int2str
id2label = {
    str(i): id2label_fn(i)
    for i in range(len(gtzan_encoded["train"].features["label"].names))
}
label2id = {v: k for k, v in id2label.items()}

"""Classification model



---


Now we will use ‘AutoModelForAudioClassification’ for the music genre classifiation. We will specify various training arguments for the model as per our choice and machine’s capability.

*   At first, we initialized a pretrained audio model for finetuning
*   We created an object containing various training configuration settings, such as evaluation strategy, learning rate, batch sizes, logging settings, etc. These settings are used during the model training process.


"""

num_labels = len(id2label)

model = AutoModelForAudioClassification.from_pretrained(
	model_id,
	num_labels=num_labels,
	label2id=label2id,
	id2label=id2label,
)

model_name = model_id.split("/")[-1]
batch_size = 2
gradient_accumulation_steps = 1
num_train_epochs = 5

training_args = TrainingArguments(
	f"{model_name}-Music classification Finetuned",
	evaluation_strategy="epoch",
	save_strategy="epoch",
	learning_rate=5e-5,
	per_device_train_batch_size=batch_size,
	gradient_accumulation_steps=gradient_accumulation_steps,
	per_device_eval_batch_size=batch_size,
	num_train_epochs=num_train_epochs,
	warmup_ratio=0.1,
	logging_steps=5,
	load_best_model_at_end=True,
	metric_for_best_model="accuracy",
	fp16=True,
)

"""Model evaluation
Now we will evaluate our model in the terms of Accuracy.


*   We loaded the accuracy metric for evaluation and it loaded from Hugging Face module.
*   We computed the evaluation metrics based on the model predictions and the reference labels. In this case, it uses the loaded accuracy metric to compute the accuracy.
*   Then we initialized the trainer and trained the model




"""

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
	predictions = np.argmax(eval_pred.predictions, axis=1)
	return metric.compute(predictions=predictions, references=eval_pred.label_ids)


trainer = Trainer(
	model,
	training_args,
	train_dataset=gtzan_encoded["train"],
	eval_dataset=gtzan_encoded["test"],
	tokenizer=feature_extractor,
	compute_metrics=compute_metrics,
)

trainer.train()

"""Loading and Saving the model

"""

# Save the model and feature extractor
model.save_pretrained("/content/Saved Model")
feature_extractor.save_pretrained("/content/Saved Model")

model.save_pretrained("/content/drive/MyDrive/Saved Model")
feature_extractor.save_pretrained("/content/drive/MyDrive/Saved Model")

"""Code for loading the model"""

# Load the model and feature extractor
loaded_model = AutoModelForAudioClassification.from_pretrained("/content/Saved Model")
loaded_feature_extractor = AutoFeatureExtractor.from_pretrained("/content/Saved Model")

"""Pipeline

---

Using this pipeline you will be able input an audio file and obtain the predicted genre along with the probability score. For the following code we have used a file of genre blue.
"""

from transformers import pipeline, AutoFeatureExtractor

pipe = pipeline("audio-classification", model=loaded_model,
				feature_extractor=loaded_feature_extractor)


def classify_audio(filepath):
	preds = pipe(filepath)
	outputs = {}
	for p in preds:
		outputs[p["label"]] = p["score"]
	return outputs


# Provide the input file path
input_file_path = input('Input:')

# Classify the audio file
output = classify_audio(input_file_path)

# Print the output genre
print("Predicted Genre:")
max_key = max(output, key=output.get)

print("The predicted genre is:", max_key)
print("The prediction score is:", output[max_key])

"""# Conclusion


---


Music genre classification presents a complex and computationally intensive challenge with broad applications across various industries. The implemented model, leveraging a DistilHuBERT-based architecture and fine-tuned on the GTZAN dataset, achieved a respectable accuracy of 82%. Further performance enhancements could be explored by utilizing a larger and more diverse dataset to improve the model's generalization capabilities and potentially achieve even higher accuracy.
"""